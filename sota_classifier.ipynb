{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sota-classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmWCmS77k+o6BseFVpbu2+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbamotra/15513/blob/master/sota_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdKrOwpeIAjC",
        "outputId": "a96b5da6-669e-40c4-dc03-be3c843b978a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsbftz1cXaeg",
        "outputId": "e65becd8-f537-40ac-d160-a2b62c666d52"
      },
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz && tar xzf food-101.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-26 04:26:00--  http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz [following]\n",
            "--2021-07-26 04:26:01--  https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4996278331 (4.7G) [application/x-gzip]\n",
            "Saving to: ‘food-101.tar.gz’\n",
            "\n",
            "food-101.tar.gz     100%[===================>]   4.65G  19.1MB/s    in 4m 17s  \n",
            "\n",
            "2021-07-26 04:30:18 (18.6 MB/s) - ‘food-101.tar.gz’ saved [4996278331/4996278331]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5khUDu5CYn2z"
      },
      "source": [
        "!find food-101 -type f | grep '.jpg$' > food101_data.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tnc79FUcYvs"
      },
      "source": [
        "!head -n 5000 food101_data.txt | shuf -n 3000 > 'small_data.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIhrrGF_Hy0e"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "kotokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIn1WqSrVZQH"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "image_tranforms = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kof7CVtvE1E6"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "class LT_Dataset(Dataset):\n",
        "    num_classes = 1000\n",
        "\n",
        "    def __init__(self, root, txt, transform=None):\n",
        "        self.img_path = []\n",
        "        self.targets = []\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(txt) as f:\n",
        "            for line in f:\n",
        "                filepath = line.split()[0]\n",
        "                self.img_path.append(os.path.join(root, filepath))\n",
        "\n",
        "                label = filepath.split('/')[2]\n",
        "                self.targets.append(label)\n",
        "        \n",
        "        # Original assigned category -> 0, ... K\n",
        "        self.label_decoder = {i: el for i, el in enumerate(np.unique(self.targets))}\n",
        "        self.label_encoder = {v: k for k, v in self.label_decoder.items()}\n",
        "\n",
        "        self.num_classes = len(self.label_encoder)\n",
        "        self.targets = [self.label_encoder[el] for el in self.targets]\n",
        "\n",
        "        cls_count_list_old = [np.sum(np.array(self.targets) == i) for i in range(self.num_classes)]\n",
        "        sorted_classes = np.argsort(-np.array(cls_count_list_old)).argsort()\n",
        "\n",
        "        # New assigned category based on descending order of sample count\n",
        "        self.count_based_label_encoder = {}\n",
        "        for i in range(self.num_classes):\n",
        "            self.count_based_label_encoder[i] = sorted_classes[i]\n",
        "        self.count_based_label_decoder = {v: k for k, v in self.count_based_label_encoder.items()}\n",
        "\n",
        "        self.targets = [self.count_based_label_encoder[target] for target in self.targets]\n",
        "        self.cls_count_list = [np.sum(np.array(self.targets)==i) for i in range(self.num_classes)]\n",
        "\n",
        "        rev_cls_count_list = self.cls_count_list[::-1]\n",
        "        \n",
        "        head_index = self.num_classes - bisect.bisect_left(rev_cls_count_list, 1000)\n",
        "        torso_index = self.num_classes - bisect.bisect_left(rev_cls_count_list, 100)\n",
        "        many_shot_index = self.num_classes - bisect.bisect_left(rev_cls_count_list, 20)\n",
        "\n",
        "        self.category_partitions = [(0, head_index), \n",
        "                                    (head_index+1, torso_index),\n",
        "                                    (torso_index+1, many_shot_index),\n",
        "                                    (many_shot_index, self.num_classes)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.img_path[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        with open(path, 'rb') as f:\n",
        "            sample = Image.open(f).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        text = '모두의 말뭉치를 추가적으로 사용하여 KoELECTRA-v3를 제작하였습니다. 제작하였습니다'\n",
        "        return sample, text, target "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taU58njLPKTl"
      },
      "source": [
        "def pad_input(input):\n",
        "    \"\"\"\n",
        "    creates a padded tensor to fit the longest sequence in the batch\n",
        "    \"\"\"\n",
        "    if len(input[0].size()) == 1:\n",
        "        l = [len(elem) for elem in input]\n",
        "        targets = torch.zeros(len(input), max(l)).long()\n",
        "        for i, elem in enumerate(input):\n",
        "            end = l[i]\n",
        "            targets[i, :end] = elem[:end]\n",
        "    else:\n",
        "        n, l = [], []\n",
        "        for elem in input:\n",
        "            n.append(elem.size(0))\n",
        "            l.append(elem.size(1))\n",
        "        targets = torch.zeros(len(input), max(n), max(l)).long()\n",
        "        for i, elem in enumerate(input):\n",
        "            targets[i, :n[i], :l[i]] = elem\n",
        "    return targets\n",
        "\n",
        "\n",
        "def collate_fn(data, tokenizer, max_length=100):\n",
        "    \"\"\" collate to consume and batchify recipe data\n",
        "    \"\"\"\n",
        "    image, texts, target = zip(*data)\n",
        "\n",
        "    image = torch.stack(image, 0)\n",
        "\n",
        "    encoded_input = tokenizer.batch_encode_plus(list(texts), \n",
        "                                                add_special_tokens=True, \n",
        "                                                max_length=max_length, \n",
        "                                                truncation=True, \n",
        "                                                return_tensors=\"pt\")\n",
        "    \n",
        "    input_ids = pad_input(encoded_input['input_ids'])\n",
        "    attention_mask = pad_input(encoded_input['attention_mask'])\n",
        "    \n",
        "    target = torch.tensor(list(target), dtype=torch.int64)\n",
        "    return image, input_ids, attention_mask, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQfGe6O1R3Ze"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znColDPcQmeR"
      },
      "source": [
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1X7roY2Qg0s"
      },
      "source": [
        "collate_fn_with_tokenizer = partial(collate_fn, tokenizer=kotokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx0Aag0tRIkO"
      },
      "source": [
        "dataset = LT_Dataset(root='.', txt='food101_data.txt', transform=image_tranforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-NFo_UyGJwD",
        "outputId": "add6957b-0aa3-4f79-caf5-cd56287b2d60"
      },
      "source": [
        "dataset[1900]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1.6042, -1.6555, -1.6727,  ..., -0.9705, -0.9363, -0.9363],\n",
              "          [-1.7412, -1.7069, -1.7069,  ..., -0.9534, -0.9363, -0.9363],\n",
              "          [-1.7754, -1.7412, -1.7240,  ..., -0.9020, -0.9020, -0.9020],\n",
              "          ...,\n",
              "          [-1.0219, -1.0562, -1.0904,  ...,  0.4166,  0.4679,  0.4337],\n",
              "          [-1.0219, -1.0562, -1.0390,  ...,  0.4166,  0.4679,  0.4679],\n",
              "          [-0.8507, -1.0048, -1.0562,  ...,  0.4166,  0.4337,  0.4508]],\n",
              " \n",
              "         [[-1.5455, -1.6155, -1.6331,  ..., -0.8803, -0.8452, -0.8452],\n",
              "          [-1.6856, -1.6681, -1.6681,  ..., -0.8627, -0.8452, -0.8452],\n",
              "          [-1.7381, -1.7031, -1.6856,  ..., -0.8102, -0.8102, -0.8102],\n",
              "          ...,\n",
              "          [-0.9678, -1.0203, -1.0553,  ...,  0.5378,  0.5903,  0.5553],\n",
              "          [-0.9503, -1.0203, -1.0028,  ...,  0.5378,  0.5903,  0.5903],\n",
              "          [-0.7577, -0.9503, -1.0203,  ...,  0.5378,  0.5553,  0.5728]],\n",
              " \n",
              "         [[-1.5256, -1.5953, -1.6127,  ..., -0.6890, -0.6541, -0.6541],\n",
              "          [-1.6302, -1.6127, -1.5953,  ..., -0.6715, -0.6541, -0.6541],\n",
              "          [-1.6650, -1.6302, -1.6127,  ..., -0.6193, -0.6193, -0.6193],\n",
              "          ...,\n",
              "          [-0.8110, -0.8284, -0.8458,  ...,  0.7228,  0.7751,  0.7402],\n",
              "          [-0.7936, -0.8284, -0.8284,  ...,  0.7228,  0.7751,  0.7751],\n",
              "          [-0.6367, -0.7936, -0.8633,  ...,  0.7228,  0.7402,  0.7576]]]),\n",
              " '모두의 말뭉치를 추가적으로 사용하여 KoELECTRA-v3를 제작하였습니다. 제작하였습니다',\n",
              " 67)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VIrD-uksTOnN",
        "outputId": "6b7b4c24-2cf5-46db-ca30-df472b29fdae"
      },
      "source": [
        "dataset.label_decoder[dataset.count_based_label_decoder[67]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beignets'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGQuFU5kH0Pn",
        "outputId": "90644481-5815-4667-d85f-307f83170dff"
      },
      "source": [
        "dataset.count_based_label_encoder[dataset.label_encoder['beignets']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-rLml-TY971"
      },
      "source": [
        "import multiprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVlcYZfQVlFZ"
      },
      "source": [
        "dl = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=64, \n",
        "            shuffle=True,\n",
        "            num_workers=multiprocessing.cpu_count(), \n",
        "            pin_memory=True,\n",
        "            collate_fn=collate_fn_with_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYCQp5lapi5K"
      },
      "source": [
        "class PrefetchLoader:\n",
        "    def __init__(self, loader):\n",
        "        self.loader = loader\n",
        "        self.stream = torch.cuda.Stream()\n",
        "\n",
        "    def __iter__(self):\n",
        "        first = True\n",
        "        for batch in self.loader:\n",
        "            with torch.cuda.stream(self.stream):\n",
        "                self.next_image = batch[0].cuda(non_blocking=True)\n",
        "                self.next_input_ids = batch[1].cuda(non_blocking=True)\n",
        "                self.next_attention_masks = batch[2].cuda(non_blocking=True)\n",
        "                self.next_target = batch[3].cuda(non_blocking=True)\n",
        "\n",
        "            if not first:\n",
        "                yield image, input_ids, attention_masks, target\n",
        "            else:\n",
        "                first = False\n",
        "\n",
        "            torch.cuda.current_stream().wait_stream(self.stream)\n",
        "            image = self.next_image\n",
        "            input_ids = self.next_input_ids\n",
        "            attention_masks = self.next_attention_masks\n",
        "            target = self.next_target\n",
        "\n",
        "            # Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.\n",
        "            image.record_stream(torch.cuda.current_stream())\n",
        "            input_ids.record_stream(torch.cuda.current_stream())\n",
        "            attention_masks.record_stream(torch.cuda.current_stream())\n",
        "            target.record_stream(torch.cuda.current_stream())\n",
        "\n",
        "        # final batch\n",
        "        yield image, input_ids, attention_masks, target\n",
        "\n",
        "        # cleaning at the end of the epoch\n",
        "        del self.next_image\n",
        "        del self.next_input_ids\n",
        "        del self.next_attention_masks\n",
        "        del self.next_target\n",
        "        \n",
        "        self.next_image = None\n",
        "        self.next_input_ids = None\n",
        "        self.next_attention_masks = None\n",
        "        self.next_target = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.loader)\n",
        "\n",
        "    @property\n",
        "    def dataset(self):\n",
        "        return self.loader.dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i93potP3rR_u"
      },
      "source": [
        "loader = PrefetchLoader(dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoDLAN76tF8a",
        "outputId": "984ed276-ee82-49e5-e4c2-a57b91558290"
      },
      "source": [
        "%%writefile gpu_usage.sh\n",
        "#! /bin/sh\n",
        "\n",
        "# Tracks GPU usage for 400 seconds, change it as per your use\n",
        "end=$((SECONDS+400))\n",
        "\n",
        "while [ $SECONDS -lt $end ]; do\n",
        "    nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu >> gpu.log\n",
        "done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting gpu_usage.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYfW_nc4tiPp",
        "outputId": "129d16a6-a9b6-4933-d354-d85d11936252"
      },
      "source": [
        "%%bash --bg\n",
        "\n",
        "bash gpu_usage.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 2 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ90Jf_jwR2T"
      },
      "source": [
        "for batch in tqdm(loader, total=len(dataset) // 64):\n",
        "  assert batch is not None, \"batch was none\"\n",
        "  assert len(batch) > 1, \"expected batch size > 1\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}